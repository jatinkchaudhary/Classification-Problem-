{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Generator_DCGAN_64x64.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jatinkchaudhary/Classification-Problem-/blob/main/Image_Generator_DCGAN_64x64.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8M9DT9ZG01v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "156a1672-6630-4aa4-f53b-40c35f43ec8c"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import datetime\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install livelossplot\n",
        "from livelossplot import PlotLossesKeras\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting livelossplot\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/f6/0618c30078f9c1e4b2cd84f1ea6bb70c6615070468b75b0d934326107bcd/livelossplot-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from livelossplot) (3.0.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from livelossplot) (5.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (2.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (1.16.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (2.5.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (5.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (2.10.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.5.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (0.8.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.4.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.6.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.5.3)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->livelossplot) (41.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->livelossplot) (1.12.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (3.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.6.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (2.1.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->livelossplot) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->livelossplot) (0.6.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->livelossplot) (2.6.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->livelossplot) (17.0.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook->livelossplot) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->livelossplot) (4.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->livelossplot) (0.5.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (4.7.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (1.0.16)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.1.7)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYT4u57aOcVU"
      },
      "source": [
        "def generator(z, output_channel_dim, training):\n",
        "    with tf.variable_scope(\"generator\", reuse= not training):\n",
        "        \n",
        "        # 8x8x512\n",
        "        fully_connected = tf.layers.dense(z, 8*8*512)\n",
        "        fully_connected = tf.reshape(fully_connected, (-1, 8, 8, 512))\n",
        "        fully_connected = tf.nn.leaky_relu(fully_connected)\n",
        "\n",
        "        # 8x8x512 -> 16x16x256\n",
        "        trans_conv1 = tf.layers.conv2d_transpose(inputs=fully_connected,\n",
        "                                                 filters=256,\n",
        "                                                 kernel_size=[3,3],\n",
        "                                                 strides=[2,2],\n",
        "                                                 padding=\"SAME\",\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                                 name=\"trans_conv1\")\n",
        "        batch_trans_conv1 = tf.layers.batch_normalization(inputs = trans_conv1,\n",
        "                                                          training=training,\n",
        "                                                          epsilon=EPSILON,\n",
        "                                                          name=\"batch_trans_conv1\")\n",
        "        trans_conv1_out = tf.nn.leaky_relu(batch_trans_conv1,\n",
        "                                           name=\"trans_conv1_out\")\n",
        "        \n",
        "        # 16x16x256 -> 32x32x128\n",
        "        trans_conv2 = tf.layers.conv2d_transpose(inputs=trans_conv1_out,\n",
        "                                                 filters=128,\n",
        "                                                 kernel_size=[5,5],\n",
        "                                                 strides=[2,2],\n",
        "                                                 padding=\"SAME\",\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                                 name=\"trans_conv2\")\n",
        "        batch_trans_conv2 = tf.layers.batch_normalization(inputs = trans_conv2,\n",
        "                                                          training=training,\n",
        "                                                          epsilon=EPSILON,\n",
        "                                                          name=\"batch_trans_conv2\")\n",
        "        trans_conv2_out = tf.nn.leaky_relu(batch_trans_conv2,\n",
        "                                           name=\"trans_conv2_out\")\n",
        "        \n",
        "        # 32x32x128 -> 64x64x64\n",
        "        trans_conv3 = tf.layers.conv2d_transpose(inputs=trans_conv2_out,\n",
        "                                                 filters=64,\n",
        "                                                 kernel_size=[3,3],\n",
        "                                                 strides=[2,2],\n",
        "                                                 padding=\"SAME\",\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                                 name=\"trans_conv3\")\n",
        "        batch_trans_conv3 = tf.layers.batch_normalization(inputs = trans_conv3,\n",
        "                                                          training=training,\n",
        "                                                          epsilon=EPSILON,\n",
        "                                                          name=\"batch_trans_conv3\")\n",
        "        trans_conv3_out = tf.nn.leaky_relu(batch_trans_conv3,\n",
        "                                           name=\"trans_conv3_out\")\n",
        "        \n",
        "        # 64x64x64 -> 64x64x3\n",
        "        logits = tf.layers.conv2d_transpose(inputs=trans_conv3_out,\n",
        "                                            filters=3,\n",
        "                                            kernel_size=[5,5],\n",
        "                                            strides=[1,1],\n",
        "                                            padding=\"SAME\",\n",
        "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                            name=\"logits\")\n",
        "        out = tf.tanh(logits, name=\"out\")\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg2k4XibOn7j"
      },
      "source": [
        "def discriminator(x, reuse):\n",
        "    with tf.variable_scope(\"discriminator\", reuse=reuse): \n",
        "        \n",
        "        # 64*64*3 -> 32x32x64 \n",
        "        conv1 = tf.layers.conv2d(inputs=x,\n",
        "                                 filters=64,\n",
        "                                 kernel_size=[5,5],\n",
        "                                 strides=[2,2],\n",
        "                                 padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                 name='conv1')\n",
        "        batch_norm1 = tf.layers.batch_normalization(conv1,\n",
        "                                                    training=True,\n",
        "                                                    epsilon=EPSILON,\n",
        "                                                    name='batch_norm1')\n",
        "        conv1_out = tf.nn.leaky_relu(batch_norm1,\n",
        "                                     name=\"conv1_out\")\n",
        "        \n",
        "        # 32x32x64-> 16x16x128 \n",
        "        conv2 = tf.layers.conv2d(inputs=conv1_out,\n",
        "                                 filters=128,\n",
        "                                 kernel_size=[5, 5],\n",
        "                                 strides=[2, 2],\n",
        "                                 padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                 name='conv2')\n",
        "        batch_norm2 = tf.layers.batch_normalization(conv2,\n",
        "                                                    training=True,\n",
        "                                                    epsilon=EPSILON,\n",
        "                                                    name='batch_norm2')\n",
        "        conv2_out = tf.nn.leaky_relu(batch_norm2,\n",
        "                                     name=\"conv2_out\")\n",
        "        \n",
        "        # 16x16x128 -> 8x8x256  \n",
        "        conv3 = tf.layers.conv2d(inputs=conv2_out,\n",
        "                                 filters=256,\n",
        "                                 kernel_size=[5, 5],\n",
        "                                 strides=[2, 2],\n",
        "                                 padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                 name='conv3')\n",
        "        batch_norm3 = tf.layers.batch_normalization(conv3,\n",
        "                                                    training=True,\n",
        "                                                    epsilon=EPSILON,\n",
        "                                                    name='batch_norm3')\n",
        "        conv3_out = tf.nn.leaky_relu(batch_norm3,\n",
        "                                     name=\"conv3_out\")\n",
        "        \n",
        "        # 8x8x256 -> 8x8x512\n",
        "        conv4 = tf.layers.conv2d(inputs=conv3_out,\n",
        "                                 filters=512,\n",
        "                                 kernel_size=[5, 5],\n",
        "                                 strides=[1, 1],\n",
        "                                 padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                 name='conv4')\n",
        "        batch_norm4 = tf.layers.batch_normalization(conv4,\n",
        "                                                    training=True,\n",
        "                                                    epsilon=EPSILON,\n",
        "                                                    name='batch_norm4')\n",
        "        conv4_out = tf.nn.leaky_relu(batch_norm4,\n",
        "                                     name=\"conv4_out\")\n",
        "\n",
        "        flatten = tf.reshape(conv4_out, (-1, 8*8*512))\n",
        "        logits = tf.layers.dense(inputs=flatten,\n",
        "                                 units=1,\n",
        "                                 activation=None)\n",
        "        out = tf.sigmoid(logits)\n",
        "        return out, logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O_5lINXOySq"
      },
      "source": [
        "def model_loss(input_real, input_z, output_channel_dim):\n",
        "    g_model = generator(input_z, output_channel_dim, True)\n",
        "\n",
        "    noisy_input_real = input_real + tf.random_normal(shape=tf.shape(input_real),\n",
        "                                                     mean=0.0,\n",
        "                                                     stddev=random.uniform(0.0, 0.1),\n",
        "                                                     dtype=tf.float32)\n",
        "    \n",
        "    d_model_real, d_logits_real = discriminator(noisy_input_real, reuse=False)\n",
        "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\n",
        "    \n",
        "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
        "                                                                         labels=tf.ones_like(d_model_real)*random.uniform(0.9, 1.0)))\n",
        "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
        "                                                                         labels=tf.zeros_like(d_model_fake)))\n",
        "    d_loss = tf.reduce_mean(0.5 * (d_loss_real + d_loss_fake))\n",
        "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
        "                                                                    labels=tf.ones_like(d_model_fake)))\n",
        "    return d_loss, g_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfVLit84O4L_"
      },
      "source": [
        "def model_optimizers(d_loss, g_loss):\n",
        "    t_vars = tf.trainable_variables()\n",
        "    g_vars = [var for var in t_vars if var.name.startswith(\"generator\")]\n",
        "    d_vars = [var for var in t_vars if var.name.startswith(\"discriminator\")]\n",
        "    \n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    gen_updates = [op for op in update_ops if op.name.startswith('generator')]\n",
        "    \n",
        "    with tf.control_dependencies(gen_updates):\n",
        "        d_train_opt = tf.train.AdamOptimizer(learning_rate=LR_D, beta1=BETA1).minimize(d_loss, var_list=d_vars)\n",
        "        g_train_opt = tf.train.AdamOptimizer(learning_rate=LR_G, beta1=BETA1).minimize(g_loss, var_list=g_vars)  \n",
        "    return d_train_opt, g_train_opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7BEI2BTO9yU"
      },
      "source": [
        "def model_inputs(real_dim, z_dim):\n",
        "    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='inputs_real')\n",
        "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name=\"input_z\")\n",
        "    learning_rate_G = tf.placeholder(tf.float32, name=\"lr_g\")\n",
        "    learning_rate_D = tf.placeholder(tf.float32, name=\"lr_d\")\n",
        "    return inputs_real, inputs_z, learning_rate_G, learning_rate_D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUWI3RVhPBtT"
      },
      "source": [
        "def show_samples(sample_images, name, epoch):\n",
        "    figure, axes = plt.subplots(1, len(sample_images), figsize = (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    for index, axis in enumerate(axes):\n",
        "        axis.axis('off')\n",
        "        image_array = sample_images[index]\n",
        "        axis.imshow(image_array)\n",
        "        image = Image.fromarray(image_array)\n",
        "        image.save(name+\"_\"+str(epoch)+\"_\"+str(index)+\".png\") \n",
        "    plt.savefig(name+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEe0FSUDPFOM"
      },
      "source": [
        "def test(sess, input_z, out_channel_dim, epoch):\n",
        "    example_z = np.random.uniform(-1, 1, size=[SAMPLES_TO_SHOW, input_z.get_shape().as_list()[-1]])\n",
        "    samples = sess.run(generator(input_z, out_channel_dim, False), feed_dict={input_z: example_z})\n",
        "    sample_images = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n",
        "    show_samples(sample_images, OUTPUT_DIR + \"samples\", epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbsc4IusPIqP"
      },
      "source": [
        "def summarize_epoch(epoch, duration, sess, d_losses, g_losses, input_z, data_shape):\n",
        "    minibatch_size = int(data_shape[0]//BATCH_SIZE)\n",
        "    print(\"Epoch {}/{}\".format(epoch, EPOCHS),\n",
        "          \"\\nDuration: {:.5f}\".format(duration),\n",
        "          \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])),\n",
        "          \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])))\n",
        "    \n",
        "    # write summary to the file\n",
        "    fptr = open(OUTPUT_DIR + \"summary.txt\", \"a\")\n",
        "    fptr.write(\"\\n----------Epoch {}/{}\".format(epoch, EPOCHS))\n",
        "    fptr.write(\"---------\")\n",
        "    fptr.write(\"\\nDuration: {:.5f}\".format(duration))\n",
        "    fptr.write(\"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])))\n",
        "    fptr.write(\"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])))\n",
        "    fptr.close()\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot(d_losses, label='Discriminator', alpha=0.6)\n",
        "    plt.plot(g_losses, label='Generator', alpha=0.6)\n",
        "    plt.title(\"Losses\")\n",
        "    plt.legend()\n",
        "    plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    test(sess, input_z, data_shape[3], epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSHu9gxKPMIy"
      },
      "source": [
        "def get_batches(data):\n",
        "    batches = []\n",
        "    for i in range(int(data.shape[0]//BATCH_SIZE)):\n",
        "        batch = data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "        augmented_images = []\n",
        "        for img in batch:\n",
        "            image = Image.fromarray(img)\n",
        "            if random.choice([True, False]):\n",
        "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            augmented_images.append(np.asarray(image))\n",
        "        batch = np.asarray(augmented_images)\n",
        "        normalized_batch = (batch / 127.5) - 1.0\n",
        "        batches.append(normalized_batch)\n",
        "    return batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iFbmx4mPP_M"
      },
      "source": [
        "def train(get_batches, data_shape, checkpoint_to_load=None):\n",
        "    input_images, input_z, lr_G, lr_D = model_inputs(data_shape[1:], NOISE_SIZE)\n",
        "    d_loss, g_loss = model_loss(input_images, input_z, data_shape[3])\n",
        "    d_opt, g_opt = model_optimizers(d_loss, g_loss)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        epoch = 0\n",
        "        iteration = 0\n",
        "        d_losses = []\n",
        "        g_losses = []\n",
        "        \n",
        "        for epoch in range(EPOCHS):        \n",
        "            epoch += 1\n",
        "            start_time = time.time()\n",
        "\n",
        "            for batch_images in get_batches:\n",
        "                iteration += 1\n",
        "                batch_z = np.random.uniform(-1, 1, size=(BATCH_SIZE, NOISE_SIZE))\n",
        "                _ = sess.run(d_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_D: LR_D})\n",
        "                _ = sess.run(g_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_G: LR_G})\n",
        "                d_losses.append(d_loss.eval({input_z: batch_z, input_images: batch_images}))\n",
        "                g_losses.append(g_loss.eval({input_z: batch_z}))\n",
        "\n",
        "            summarize_epoch(epoch, time.time()-start_time, sess, d_losses, g_losses, input_z, data_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRLWcPZFSuFw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1590dcb-0240-4fd9-f075-aaea32da1bd6"
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fybMg-wUPVGU"
      },
      "source": [
        "# Paths\n",
        "INPUT_DATA_DIR = \"/content/drive/My Drive/Colab Notebooks/data/cats_64x64/inputs/\" # Path to the folder with input images. For more info check simspons_dataset.txt\n",
        "OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/data/cats_64x64/outputs/{date:%Y-%m-%d_%H:%M:%S}/'.format(date=datetime.datetime.now())\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ubophYPbpO"
      },
      "source": [
        "# Hyperparameters\n",
        "IMAGE_SIZE = 64\n",
        "NOISE_SIZE = 100\n",
        "LR_D = 0.00004\n",
        "LR_G = 0.0004\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 300\n",
        "BETA1 = 0.5\n",
        "WEIGHT_INIT_STDDEV = 0.02\n",
        "EPSILON = 0.00005\n",
        "SAMPLES_TO_SHOW = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bin1f5lgPe1q"
      },
      "source": [
        "# Training\n",
        "input_images = np.asarray([np.asarray(Image.open(file).resize((IMAGE_SIZE, IMAGE_SIZE))) for file in glob(INPUT_DATA_DIR + '*')])\n",
        "\n",
        "np.random.shuffle(input_images)\n",
        "\n",
        "sample_images = random.sample(list(input_images), SAMPLES_TO_SHOW)\n",
        "show_samples(sample_images, OUTPUT_DIR + \"inputs\", 0)\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    train(get_batches(input_images), input_images.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrICCioBrklS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06c6ea5b-f54c-49a1-b4cd-997f73e14ec4"
      },
      "source": [
        "!cat /proc/cpuinfo. \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: /proc/cpuinfo.: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}